{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 实战 Google 深度学习框架 | 学习笔记（二）\n",
    "\n",
    ">Shoot on the moon and if you miss you will still be among the stars.\n",
    "\n",
    "Caicloud Github :[tensorflow-tutorial](https://github.com/caicloud/tensorflow-tutorial)https://github.com/caicloud/tensorflow-tutorial\n",
    "\n",
    "原 tutorial 使用的 Tensorflow 最新版本是 1.4.0 ，本人使用的 1.5.0 版本，所以部分代码会略有不同，该笔记仅为个人学习，理解使用。如有错误，还望批评指教。----ZJ\n",
    "\n",
    "## 4 深层神经网络\n",
    "\n",
    "### 4.1 深度学习与深层神经网络\n",
    "\n",
    "wiki 深度学习定义：一类通过多层非线性变换对高复杂性数据建模算法的合集。\n",
    "\n",
    "深度学习，两个重要特性：**多层 and 非线性。**\n",
    "\n",
    "- 线性模型的局限性，线性模型能够解决的问题是有限的(解决线性可分的问题)。\n",
    "    - 3.4.2 小节中，输出为所有输入的加权和，导致整个神经网络是一个线性模型。\n",
    "    - （采用线性函数）任意层的全连接神经网络（FC）和单层神经网络模型的表达能力没有任何区别。\n",
    "    - 线性模型可以解决线性可分的问题，如通过直线（或高维空间的平面）划分。\n",
    "    - 现实世界中，绝大多数问题是无法线性分割的，所以出现 Relu 这样非线性的激活函数，来解决高复杂性的问题（深度学习的目的就是解决这样的问题）\n",
    "    \n",
    "- 激活函数实现去线性化\n",
    "    - 如果每个神经元的输出通过一个非线性函数，那么整个神经网络就不再是线性的了。\n",
    "    - 添加偏置项（bias）,偏置项 是神经网络中非常有用的一种结构。\n",
    "    - 每个节点（神经元）的输出在 加权和的基础上，还做了一个非线性变换。\n",
    "    - 常用非线性激活函数（tf.nn.relu, tf.sigmoid, tf.tanh）\n",
    "\n",
    "- 多层网络解决异或运算\n",
    "    - 在神经网络的发展史，有个很重要的问题，**异或问题**。\n",
    "    - 1958 感知机模型，将输入进行加权和，没有隐藏层，然后经过激活函数最后得到输出。不能解决异或问题。\n",
    "    - 异或问题（直观理解）：两个输入的符号相同时（同正，同负）输出为 0 ，否则（一正一负）输出为 1.\n",
    "    - 加入隐藏层后，异或问题得到很好的解决\n",
    "    - 深层网络有组合特征提取的功能，这个特性对于解决不易提取特征向量的问题（如图片识别，语音识别等）有很大的帮助。\n",
    "\n",
    "### 4.2 损失函数的定义\n",
    "\n",
    "**神经网络模型的效果以及优化的目标是通过损失函数（loss function）来定义的。**\n",
    "\n",
    "监督学习的两大种类：分类问题 and 回归问题。\n",
    "\n",
    "- 经典损失函数\n",
    "\n",
    "- 自定义损失函数\n",
    "\n",
    "\n",
    "如何判断一个输出向量和期望的向量有多接近？------交叉熵（cross entropy）是最常用的评判方法之一。\n",
    "\n",
    "交叉熵 （cross entropy）:\n",
    "\n",
    "给定两个概率分布 p,q \n",
    "\n",
    "$$H(p,q)= - \\sum_{x}p(x)logq(x)$$\n",
    "\n",
    "注意：交叉熵刻画的是两个概率之间的分布，但是神经网络的输出不一定是一个概率分布。\n",
    "\n",
    "- 概率分布刻画了不同事件发生的概率，当事件总数有限的情况下，概率分布函数 $p(X=x)$ 满足：\n",
    "\n",
    "$$\\forall x \\space\\space p(X=x)\\in[0,1]  且 \\sum_{x}p(X=x)=1$$\n",
    "\n",
    "- 任意事件发生的概率都在 0 和 1 之间\n",
    "- 且总有某一个事件发生（概率的和为 1）\n",
    "\n",
    "如何将神经网络前向传播的结果变成概率分布？\n",
    "\n",
    "- softmax 回归是一个常用的方法\n",
    "\n",
    "【图】4_10\n",
    "\n",
    "假设原始的神经网络输出为 $y_1,y_2,\\dots,y_n$,那么经过 softmax 处理之后的输出为：\n",
    "\n",
    "$$ softmax(y)_i=y_i^{'}=\\frac{e^{yi}}{\\sum_{j=1}^{n}e^{yj}}$$\n",
    "\n",
    "- 原始神经网络的输出被用作置信度来生成新的输出,新的输出满足概率分布的所有要求\n",
    "- 新的输出可以理解为经过神经网络的推导，一个样本为不同类别的概率分别有多大\n",
    "- 因此神经网络的输出变为了一个概率分布，从而可以通过交叉熵来计算预测概率分布和真实答案的概率分布之间的距离。\n",
    "- 交叉熵函数不是对称的 （$H(p.q)\\neq H(q,p)$）,刻画的是通过概率 q 来表达概率分布 p 的困难程度\n",
    "- 在神经网络中，p 代表正确标签，q 代表预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tensorflow 实现交叉熵\n",
    "\n",
    "'''\n",
    "\n",
    "cross_entropy = -tf.reduce_mean(y* tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce_mean 可以理解为 $ \\frac{1}{m} \\sum$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
